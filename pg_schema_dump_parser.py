#!/usr/bin/env python3

import os
import logging
import re
import argparse
import subprocess
import configparser
import shutil
from datetime import datetime, timezone
from time import time


APPLICATION_NAME = 'pg_schema_dump_parser'
logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', encoding='utf-8', level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S")
logger = logging.getLogger(APPLICATION_NAME)


def generate_metadata(directory: str, host: str, dbname: str, port: str, user: str, password: str, elapsed_time: str) -> str:
    """ Generates metadata """
    database_version = subprocess.Popen(
        ['psql',
        f"--dbname=postgresql://{user}:{password}@{host}:{port}/{dbname}?application_name={APPLICATION_NAME}",
         "-A",
         "--no-align",
         "--no-psqlrc",
         "--tuples-only",
         f"-c SELECT setting FROM pg_catalog.pg_settings WHERE name = 'server_version'"],
        stdout=subprocess.PIPE
    )  # pylint: disable=R1732

    pg_dump_version = subprocess.Popen(
        ['pg_dump',
         "--version",
         ],
        stdout=subprocess.PIPE
    )  # pylint: disable=R1732

    database_name = f"database_name: {dbname}"
    database_host = f"database_host: {host}"
    file_name = f"{directory}/schema/METADATA"
    database_version = f"database_version: {database_version.communicate()[0].decode('utf-8').strip()}"
    pg_dump_version = re.search(r"([0-9]*[.]?[0-9]+)", pg_dump_version.communicate()[0].decode('utf-8').strip()).group(1)
    pg_dump_version = f"pg_dump_version: {pg_dump_version}"
    if not os.path.exists(file_name):
        with open(file_name, 'a', encoding='utf-8') as file:
            file.write('# Do not edit\n' + f"# Generated by {APPLICATION_NAME} " + str(datetime.now(timezone.utc)) + f"\n# Schema parsing completed in {elapsed_time}\n\n")
            file.write(database_version + '\n')
            file.write(pg_dump_version + '\n')
            file.write(database_name + '\n')
            file.write(database_host + '\n')


def read_in_chunk(stream: str, separator: str) -> str:
    """ Read in chunk https://stackoverflow.com/questions/47927039/reading-a-file-until-a-specific-character-in-python """
    buffer = ''
    while True:  # until EOF
        chunk = stream.readline(4096)  # 4096
        if not chunk:  # EOF?
            yield buffer
            break
        buffer += chunk
        while True:  # until no separator is found
            try:
                part, buffer = buffer.split(separator, 1)
            except ValueError:
                break
            else:
                yield part


def pg_schema_dump(host: str, dbname: str, port: str, user: str, password: str) -> str:
    """ Get schema dump of a postgres database """

    pg_dump_proc = subprocess.Popen(
        ['pg_dump',
         f"--dbname=postgresql://{user}:{password}@{host}:{port}/{dbname}?application_name={APPLICATION_NAME}",
         "--schema-only",
         # '-f', dump_file,
         ],
        stdout=subprocess.PIPE
    )  # pylint: disable=R1732
    # clean up SET and SQL comments
    modified_dump = subprocess.Popen(['sed', '/^--/d;/^\\s*$/d;/^SET/d'], text=True, stdin=pg_dump_proc.stdout, stdout=subprocess.PIPE)  # pylint: disable=R1732
    return modified_dump.stdout


def parse_schema(directory: str, object_type: str, schema: str, object_name: str, definition: str, append: bool) -> None:
    """ Writes or appends to schema file """

    dir_path = f"{directory}/schema/{object_type}/{schema}"

    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

    file_name = f"{dir_path}/{object_name}.sql"

    if append:
        if not os.path.exists(file_name):
            with open(file_name, 'a', encoding='utf-8') as file:
                file.write(definition)
        else:
            with open(file_name, 'r+', encoding='utf-8') as file:
                current_content = [e+';\n' for e in read_in_chunk(file, ';\n') if e]
                line_found = any(definition in line for line in current_content)
                # if definition does not exist, append it to the schema file
                if not line_found:
                    file.seek(0, os.SEEK_END)
                    file.write(definition)
    else:
        with open(file_name, 'w', encoding='utf-8') as file:
            file.write(definition)


def parse_object(stream: str, object_type: str, append: bool = False) -> None:
    """ Parses tables, views, materialized views, sequences, types, aggregates, defaults, constraints, rules, triggers, clustered indexes, comments, extensions, foreign tables, partitions """

    schema_name = re.match(r"^(CREATE.*TABLE|COMMENT ON \w+|CREATE AGGREGATE|CREATE.*VIEW|CREATE TYPE|CREATE DOMAIN|CREATE SEQUENCE|ALTER.*TABLE \w+|ALTER.*TABLE|.*TRIGGER.*?ON|.*RULE.*\n.*?ON.*) (\w+).(\w+)", stream, re.I).group(2)
    object_name = re.match(r"^(CREATE.*TABLE|COMMENT ON \w+|CREATE AGGREGATE|CREATE.*VIEW|CREATE TYPE|CREATE DOMAIN|CREATE SEQUENCE|ALTER.*TABLE \w+|ALTER.*TABLE|.*TRIGGER.*?ON|.*RULE.*\n.*?ON.*) (\w+).(\w+)", stream, re.I).group(3)
    parse_schema(args.directory, object_type, schema_name, object_name, stream, append)


def parse_indexes(stream: str, object_type: str, append: bool = False) -> None:
    """ Parses indexes """
    index_name = re.match(r"^CREATE .*INDEX (\w+) ON (\w+).(\w+)", stream, re.I).group(1)
    schema_name = re.match(r"^CREATE .*INDEX (\w+) ON (\w+).(\w+)", stream, re.I).group(2)
    parse_schema(args.directory, object_type, schema_name, index_name, stream, append)


def parse_extensions(stream: str, object_type: str, append: bool = False) -> None:
    """ Parses extensions """
    extension_name = re.match(r"^CREATE EXTENSION.* (\w+) WITH SCHEMA (\w+)", stream, re.I).group(1)
    schema_name = re.match(r"^CREATE EXTENSION.* (\w+) WITH SCHEMA (\w+)", stream, re.I).group(2)
    parse_schema(args.directory, object_type, schema_name, extension_name, stream, append)


def parse_function(stream: str, object_type: str, append: bool = False) -> None:
    """ Parses function and procedure definition """

    # see https://www.geeksforgeeks.org/postgresql-dollar-quoted-string-constants/
    # because PG functions' bodies can be written as dollar quotes and single quotes
    # we rely solely on pg_get_functiondef for parsing functions

    host = config.get('postgresql', 'host')
    port = config.get('postgresql', 'port')
    dbname = config.get('postgresql', 'db')
    user = config.get('postgresql', 'user')
    password = config.get('postgresql', 'password')

    schema_name = re.match(r"^(CREATE FUNCTION|CREATE OR REPLACE FUNCTION|CREATE PROCEDURE|CREATE OR REPLACE PROCEDURE) (\w+).(\w+)", stream, re.I).group(2)
    func_name = re.match(r"^(CREATE FUNCTION|CREATE OR REPLACE FUNCTION|CREATE PROCEDURE|CREATE OR REPLACE PROCEDURE) (\w+).(\w+)", stream, re.I).group(3)

    with subprocess.Popen(
        ['psql',
         f"--dbname=postgresql://{user}:{password}@{host}:{port}/{dbname}?application_name={APPLICATION_NAME}",
         "-A",
         "--no-align",
         "--no-psqlrc",
         "--tuples-only",
         f"-c SELECT pg_catalog.string_agg(pg_catalog.pg_get_functiondef(f.oid), E';\n') || ';' AS def FROM (SELECT oid \
             FROM pg_catalog.pg_proc WHERE proname = '{func_name}' AND pronamespace = '{schema_name}'::regnamespace) AS f"],
        stdout=subprocess.PIPE
         ) as func_def_proc:

        func_def = func_def_proc.communicate()[0].decode('utf-8').strip()

        parse_schema(args.directory, object_type, schema_name, func_name, func_def + '\n', append)


def parse_utility(stream: str, utility_type: str, append: bool = True) -> None:
    """ Parses utilitities such as triggers, ownerships, acls, comments, mappings, schemas, rules, events, servers """

    parse_schema(args.directory, 'utilities', 'others', utility_type, stream, append)


obj_types = {
    'tables': {
        'parser': parse_object,
        'tags': (
            'CREATE TABLE',
            'CREATE UNLOGGED TABLE',
            'CREATE FOREIGN TABLE',
            ),
        'includes': {
            'tables': [''],
            },
        'append': False,
        },
    'columns_mod': {
        'parser': parse_object,
        'tags': (
            'ALTER TABLE',
            'ALTER FOREIGN TABLE',
        ),
        'includes': {
            'columns_mod': ['ALTER COLUMN'],
            'clustered_indexes': ['CLUSTER ON'],
            'constraints': ['ADD CONSTRAINT'],
            'defaults': ['SET DEFAULT'],
            'partitions': ['ATTACH PARTITION', 'INHERIT'],
            'identities': ['ADD GENERATED ALWAYS AS IDENTITY'],
            'replica_identities': ['REPLICA IDENTITY'],
            'row_level_securities': ['ROW LEVEL SECURITY'],
            },
        'append': True,
        },
    'indexes': {
        'parser': parse_indexes,
        'tags': (
            'CREATE INDEX',
            'CREATE UNIQUE INDEX',
        ),
        'includes': {
            'indexes': [''],
        },
        'append': False
    },
    'views': {
        'parser': parse_object,
        'tags': (
            'CREATE VIEW',
            'CREATE OR REPLACE VIEW',
            'CREATE MATERIALIZED VIEW',
        ),
        'includes': {
            'views': [''],
        },
        'append': True
    },
        'aggregates': {
            'parser': parse_object,
            'tags': (
                'CREATE AGGREGATE',
                'CREATE OR REPLACE AGGREGATE',
            ),
            'includes': {
                'aggregates': [''],
            },
            'append': True
        },
        'types': {
            'parser': parse_object,
            'tags': (
                'CREATE TYPE',
            ),
            'includes': {
                'types': [''],
            },
            'append': False
        },
        'sequences': {
            'parser': parse_object,
            'tags': (
                'CREATE SEQUENCE',
            ),
            'includes': {
                'sequences': [''],
            },
            'append': False
        },
        'schemas': {
            'parser': parse_utility,
            'tags': (
                'CREATE SCHEMA',
            ),
            'includes': {
                'schemas': [''],
            },
            'append': True
        },
        'extensions': {
            'parser': parse_extensions,
            'tags': (
                'CREATE EXTENSION',
            ),
            'includes': {
                'extensions': [''],
            },
            'append': False
        },
        'servers': {
            'parser': parse_utility,
            'tags': (
                'CREATE SERVER',
            ),
            'includes': {
                'servers': [''],
            },
            'append': True
        },
        'events': {
            'parser': parse_utility,
            'tags': (
                'CREATE EVENT TRIGGER',
                'ALTER EVENT TRIGGER',
            ),
            'includes': {
                'events': [
                    'DISABLE',
                    'CREATE EVENT TRIGGER',
                    ],
            },
            'append': True
        },
        'mappings': {
            'parser': parse_utility,
            'tags': (
                'CREATE USER MAPPING',
            ),
            'includes': {
                'mappings': [''],
            },
            'append': True
        },
        'publications': {
            'parser': parse_utility,
            'tags': (
                'CREATE PUBLICATION',
                'ALTER PUBLICATION'
            ),
            'includes': {
                'publications': [
                    'CREATE PUBLICATION',
                    'ADD'
                    ],
            },
            'append': True
        },
        'subscriptions': {
            'parser': parse_utility,
            'tags': (
                'CREATE SUBSCRIPTION',
                'ALTER SUBSCRIPTION',
            ),
            'includes': {
                'subscriptions': [
                    'CREATE SUBSCRIPTION',
                    'ADD',
                    ],
            },
            'append': True
        },
        'ownerships': {
            'parser': parse_utility,
            'tags': (
                '',
            ),
            'includes': {
                'ownerships': [
                    'OWNER TO',
                    'OWNED BY',
                    ],
            },
            'append': True
        },
        'acls': {
            'parser': parse_utility,
            'tags': (
                '',
            ),
            'includes': {
                'acls': [
                    'GRANT',
                    'REVOKE',
                    ],
            },
            'append': True
        },
        'comments': {
            'parser': parse_utility,
            'tags': (
                'COMMENT',
            ),
            'includes': {
                'comments': [
                    'COMMENT',
                    'ADD',
                    ],
            },
            'append': True
        },
        'functions': {
            'parser': parse_function,
            'tags': (
                'CREATE FUNCTION',
                'CREATE OR REPLACE FUNCTION',
                'CREATE PROCEDURE',
                'CREATE OR REPLACE PROCEDURE',
            ),
            'includes': {
                'functions': [
                    '',
                    ],
            },
            'append': False
        },
        #  NOTE: triggers and rules are a bit different from the other objects due to their syntaxes,
        #  we can have ALTER TABLE *** TRIGGER/ALTER TABLE *** RULE
        #  the current best solution is to move all tags to includes
        'triggers': {
            'parser': parse_object,
            'tags': (
                '',
            ),
            'includes': {
                'triggers': [
                    'CREATE TRIGGER',
                    'CREATE OR REPLACE TRIGGER',
                    'CREATE CONSTRAINT TRIGGER',
                    'CREATE OR REPLACE CONSTRAINT TRIGGER',
                    'ALTER TRIGGER',
                    'DISABLE TRIGGER',
                    'ENABLE TRIGGER',
                    'ENABLE ALWAYS TRIGGER',
                    'ENABLE REPLICA TRIGGER',
                    ],
            },
            'append': True
        },
        'rules': {
            'parser': parse_object,
            'tags': (
                '',
            ),
            'includes': {
                'rules': [
                    'CREATE RULE',
                    'CREATE OR REPLACE RULE',
                    'ALTER RULE',
                    'DISABLE RULE',
                    ],
            },
            'append': True
        },
        'domains': {
            'parser': parse_object,
            'tags': (
                'CREATE DOMAIN',
            ),
            'includes': {
                'domains': [''],
            },
            'append': False
        },
    }

#  TODO: in a case a table depends on a user-defined function, we can simply add a dummy function before the create table


if __name__ == "__main__":
    file_path = os.path.abspath(__file__)
    args_parser = argparse.ArgumentParser(
        description="""Generates nicely parsed schema files""",
        epilog=f"example: {file_path} --directory . --configfile pg_schema_dump.config",
                    formatter_class=argparse.RawDescriptionHelpFormatter)
    args_parser.add_argument('--directory', required=True, help="Directory to drop the schema files into")
    args_parser.add_argument('--configfile', required=True, help="Database configuration file, see sample")
    args = args_parser.parse_args()

    config = configparser.ConfigParser()
    config.read(args.configfile)

    postgres_host = config.get('postgresql', 'host')
    postgres_port = config.get('postgresql', 'port')
    postgres_db = config.get('postgresql', 'db')
    postgres_user = config.get('postgresql', 'user')
    postgres_password = config.get('postgresql', 'password')

    # clean up previous parse if it exists
    if os.path.exists(f"{args.directory}/schema"):
        shutil.rmtree(f"{args.directory}/schema")

    start_time = time()

    with pg_schema_dump(postgres_host, postgres_db, postgres_port, postgres_user, postgres_password) as f:
        logger.info(f"Started parser: {APPLICATION_NAME}")
        for segment in read_in_chunk(f, separator=';\n'):
            if segment:
                segment = segment + ';\n'
            for obj_type, value in obj_types.items():
                for op, includes in value.get('includes').items():
                    for include in includes:
                        if segment.startswith((value.get('tags'))) and (include in segment):
                            value.get('parser')(segment, op, value.get('append'))
    elapsed_time = f"{(time() - start_time):.2f} seconds"
    generate_metadata(args.directory, postgres_host, postgres_db, postgres_port, postgres_user, postgres_password, elapsed_time)
    logger.info("Schema parsing completed in %s", elapsed_time)
